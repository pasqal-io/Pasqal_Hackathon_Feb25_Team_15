{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 179102,
     "status": "ok",
     "timestamp": 1741548181152,
     "user": {
      "displayName": "Vincenzo Schiano",
      "userId": "04691900049132186889"
     },
     "user_tz": -60
    },
    "id": "OKvXvthD9Krb",
    "outputId": "64604652-295d-41fb-f844-a65355f34695"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Using cached numpy-2.2.3-cp313-cp313-macosx_10_13_x86_64.whl.metadata (62 kB)\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.2.3-cp313-cp313-macosx_10_13_x86_64.whl.metadata (89 kB)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.6.1-cp313-cp313-macosx_10_13_x86_64.whl.metadata (31 kB)\n",
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.10.1-cp313-cp313-macosx_10_13_x86_64.whl.metadata (11 kB)\n",
      "Collecting seaborn\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting pytorch\n",
      "  Downloading pytorch-1.0.2.tar.gz (689 bytes)\n",
      "  Installing build dependencies ... \u001b[?2done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting qadence\n",
      "  Using cached qadence-1.11.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pulser\n",
      "  Using cached pulser-1.3.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/gm/Projects/Pasqal_Hackathon_Feb25_Team_15/venv/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.15.2-cp313-cp313-macosx_10_13_x86_64.whl.metadata (61 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.1-cp313-cp313-macosx_10_13_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.56.0-cp313-cp313-macosx_10_13_x86_64.whl.metadata (101 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.8-cp313-cp313-macosx_10_13_x86_64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/gm/Projects/Pasqal_Hackathon_Feb25_Team_15/venv/lib/python3.13/site-packages (from matplotlib) (24.2)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Downloading pillow-11.1.0-cp313-cp313-macosx_10_13_x86_64.whl.metadata (9.1 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.2.1-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting arpeggio==2.0.2 (from qadence)\n",
      "  Downloading Arpeggio-2.0.2-py2.py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting deepdiff (from qadence)\n",
      "  Downloading deepdiff-8.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: jsonschema in /Users/gm/Projects/Pasqal_Hackathon_Feb25_Team_15/venv/lib/python3.13/site-packages (from qadence) (4.23.0)\n",
      "Collecting nevergrad (from qadence)\n",
      "  Downloading nevergrad-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting openfermion (from qadence)\n",
      "  Downloading openfermion-1.7.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pasqal-cloud (from qadence)\n",
      "  Downloading pasqal_cloud-0.20.2-py3-none-any.whl.metadata (13 kB)\n",
      "INFO: pip is looking at multiple versions of qadence to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting qadence\n",
      "  Downloading qadence-1.10.3-py3-none-any.whl.metadata (10 kB)\n",
      "  Downloading qadence-1.10.2-py3-none-any.whl.metadata (10 kB)\n",
      "  Downloading qadence-1.10.1-py3-none-any.whl.metadata (10 kB)\n",
      "  Downloading qadence-1.10.0-py3-none-any.whl.metadata (10.0 kB)\n",
      "  Downloading qadence-1.9.2-py3-none-any.whl.metadata (10.0 kB)\n",
      "  Downloading qadence-1.9.1-py3-none-any.whl.metadata (9.8 kB)\n",
      "  Downloading qadence-1.9.0-py3-none-any.whl.metadata (9.8 kB)\n",
      "INFO: pip is still looking at multiple versions of qadence to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading qadence-1.8.0-py3-none-any.whl.metadata (9.9 kB)\n",
      "  Downloading qadence-1.7.8-py3-none-any.whl.metadata (10.0 kB)\n",
      "  Downloading qadence-1.7.7-py3-none-any.whl.metadata (10.0 kB)\n",
      "  Downloading qadence-1.7.6-py3-none-any.whl.metadata (9.9 kB)\n",
      "  Downloading qadence-1.7.5-py3-none-any.whl.metadata (9.9 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading qadence-1.7.4-py3-none-any.whl.metadata (9.9 kB)\n",
      "  Downloading qadence-1.7.3-py3-none-any.whl.metadata (9.9 kB)\n",
      "  Downloading qadence-1.7.2-py3-none-any.whl.metadata (9.8 kB)\n",
      "  Downloading qadence-1.7.1-py3-none-any.whl.metadata (9.8 kB)\n",
      "  Downloading qadence-1.7.0-py3-none-any.whl.metadata (9.6 kB)\n",
      "  Downloading qadence-1.6.3-py3-none-any.whl.metadata (9.6 kB)\n",
      "  Downloading qadence-1.6.2-py3-none-any.whl.metadata (9.2 kB)\n",
      "  Downloading qadence-1.6.1-py3-none-any.whl.metadata (9.2 kB)\n",
      "  Downloading qadence-1.6.0-py3-none-any.whl.metadata (9.3 kB)\n",
      "\u001b[31mERROR: Cannot install qadence==1.10.0, qadence==1.10.1, qadence==1.10.2, qadence==1.10.3, qadence==1.11.0, qadence==1.6.0, qadence==1.6.1, qadence==1.6.2, qadence==1.6.3, qadence==1.7.0, qadence==1.7.1, qadence==1.7.2, qadence==1.7.3, qadence==1.7.4, qadence==1.7.5, qadence==1.7.6, qadence==1.7.7, qadence==1.7.8, qadence==1.8.0, qadence==1.9.0, qadence==1.9.1 and qadence==1.9.2 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "The conflict is caused by:\n",
      "    qadence 1.11.0 depends on pyqtorch==1.7.1\n",
      "    qadence 1.10.3 depends on pyqtorch==1.7.0\n",
      "    qadence 1.10.2 depends on pyqtorch==1.7.0\n",
      "    qadence 1.10.1 depends on pyqtorch==1.7.0\n",
      "    qadence 1.10.0 depends on pyqtorch==1.7.0\n",
      "    qadence 1.9.2 depends on pyqtorch==1.7.0\n",
      "    qadence 1.9.1 depends on pyqtorch==1.6.0\n",
      "    qadence 1.9.0 depends on pyqtorch==1.6.0\n",
      "    qadence 1.8.0 depends on pyqtorch==1.5.1\n",
      "    qadence 1.7.8 depends on pyqtorch==1.4.9\n",
      "    qadence 1.7.7 depends on pyqtorch==1.4.7\n",
      "    qadence 1.7.6 depends on pyqtorch==1.4.4\n",
      "    qadence 1.7.5 depends on pyqtorch==1.4.3\n",
      "    qadence 1.7.4 depends on pyqtorch==1.3.2\n",
      "    qadence 1.7.3 depends on pyqtorch==1.3.2\n",
      "    qadence 1.7.2 depends on pyqtorch==1.2.5\n",
      "    qadence 1.7.1 depends on pyqtorch==1.2.5\n",
      "    qadence 1.7.0 depends on pyqtorch==1.2.1\n",
      "    qadence 1.6.3 depends on pyqtorch==1.2.1\n",
      "    qadence 1.6.2 depends on pyqtorch==1.1.2\n",
      "    qadence 1.6.1 depends on pyqtorch==1.1.2\n",
      "    qadence 1.6.0 depends on pyqtorch==1.1.2\n",
      "\n",
      "To fix this you could try to:\n",
      "1. loosen the range of package versions you've specified\n",
      "2. remove package versions to allow pip to attempt to solve the dependency conflict\n",
      "\n",
      "\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement quantum-evolution-kernel (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[31mERROR: No matching distribution found for quantum-evolution-kernel\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install numpy pandas scikit-learn matplotlib seaborn pytorch qadence pulser\n",
    "!pip install quantum-evolution-kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZFQPP86QyB_c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11545,
     "status": "ok",
     "timestamp": 1741547925452,
     "user": {
      "displayName": "Vincenzo Schiano",
      "userId": "04691900049132186889"
     },
     "user_tz": -60
    },
    "id": "eSuA_ind_xdD",
    "outputId": "573cf1f3-14d6-4fea-c22e-bdfd811efd18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coral Reef Bleaching Prediction using Classical and Quantum ML Approaches\n",
      "Loading data from online URL: https://raw.githubusercontent.com/alyshapm/coral-reef-bleaching/main/dataset/NOAA_Reef_Check__Bleaching_Data.csv\n",
      "Original dataset shape: (9111, 12)\n",
      "  Bleaching     Ocean  Year  Depth Storms HumanImpact     Siltation Dynamite  \\\n",
      "0        No  Atlantic  2005    4.0    yes        high         often     none   \n",
      "1        No   Red Sea  2004    6.0     no        high  occasionally     none   \n",
      "2        No   Pacific  1998    3.0     no         low         never     none   \n",
      "3        No   Pacific  1998   10.0     no         low         never     none   \n",
      "4        No  Atlantic  1997   10.0     no        high         never     none   \n",
      "\n",
      "  Poison Sewage Industrial Commercial  \n",
      "0   none   high       none       none  \n",
      "1   none    low       none       none  \n",
      "2   none   none        low       none  \n",
      "3   none   none        low       none  \n",
      "4   none   high   moderate       none  \n",
      "\n",
      "Class distribution before cleaning:\n",
      "Bleaching\n",
      "No     6480\n",
      "Yes     219\n",
      "Name: count, dtype: int64\n",
      "Positive rate: 3.27%\n",
      "Shape after handling missing values: (6699, 12)\n",
      "\n",
      "Column data types:\n",
      "Bleaching       object\n",
      "Ocean           object\n",
      "Year             int64\n",
      "Depth          float64\n",
      "Storms          object\n",
      "HumanImpact     object\n",
      "Siltation       object\n",
      "Dynamite        object\n",
      "Poison          object\n",
      "Sewage          object\n",
      "Industrial      object\n",
      "Commercial      object\n",
      "dtype: object\n",
      "\n",
      "Encoding categorical columns...\n",
      "Encoded Bleaching to 1/0\n",
      "Encoded Storms to 1/0\n",
      "Encoded Ocean column\n",
      "Unique values in Commercial before mapping: ['none' 'high' 'low' 'moderate']\n",
      "Encoded Commercial column\n",
      "Unique values in HumanImpact before mapping: ['high' 'low' 'none' 'moderate']\n",
      "Encoded HumanImpact column\n",
      "Unique values in Siltation before mapping: ['often' 'occasionally' 'never' 'always']\n",
      "Warning: Column Siltation has 6699 NaN values after mapping\n",
      "Unique values in Siltation after mapping: [nan]\n",
      "Filled NaN values in Siltation with 0\n",
      "Encoded Siltation column\n",
      "Unique values in Dynamite before mapping: ['none' 'low' 'moderate' 'high']\n",
      "Encoded Dynamite column\n",
      "Unique values in Poison before mapping: ['none' 'low' 'moderate' 'high']\n",
      "Encoded Poison column\n",
      "Unique values in Sewage before mapping: ['high' 'low' 'none' 'moderate']\n",
      "Encoded Sewage column\n",
      "Unique values in Industrial before mapping: ['none' 'low' 'moderate' 'high']\n",
      "Encoded Industrial column\n",
      "\n",
      "Warning: Dataset still contains NaN values:\n",
      "Ocean    15\n",
      "dtype: int64\n",
      "Filling remaining NaN values...\n",
      "\n",
      "Correlations with Bleaching:\n",
      "Correlation of Ocean with Bleaching: 0.0090\n",
      "Correlation of Year with Bleaching: 0.2670\n",
      "Correlation of Depth with Bleaching: 0.0060\n",
      "Correlation of Storms with Bleaching: 0.0224\n",
      "Correlation of HumanImpact with Bleaching: 0.0033\n",
      "Correlation of Siltation with Bleaching: nan\n",
      "Correlation of Dynamite with Bleaching: 0.0336\n",
      "Correlation of Poison with Bleaching: 0.0220\n",
      "Correlation of Sewage with Bleaching: 0.0017\n",
      "Correlation of Industrial with Bleaching: 0.0141\n",
      "Correlation of Commercial with Bleaching: 0.1454\n",
      "Dropping low correlation features: ['Ocean', 'Depth', 'Storms', 'HumanImpact', 'Dynamite', 'Poison', 'Sewage', 'Industrial']\n",
      "\n",
      "Class distribution after preprocessing:\n",
      "Bleaching\n",
      "0    6480\n",
      "1     219\n",
      "Name: count, dtype: int64\n",
      "Positive rate: 3.27%\n",
      "Dataset is imbalanced. Using stratified sampling...\n",
      "Training data shape: (5024, 3)\n",
      "Test data shape: (1675, 3)\n",
      "Training set positive rate: 3.26%\n",
      "Test set positive rate: 3.28%\n",
      "Minority class ratio: 3.26%\n",
      "Using class weight 'balanced' for models due to imbalanced data\n",
      "\n",
      "----- Classical Model Results -----\n",
      "Logistic Regression Results:\n",
      "  Accuracy: 0.8621\n",
      "  Balanced Accuracy: 0.9199\n",
      "  Precision: 0.1901\n",
      "  Recall: 0.9818\n",
      "  F1 Score: 0.3186\n",
      "  Training Time: 0.0149 seconds\n",
      "  Confusion Matrix:\n",
      "[[1390  230]\n",
      " [   1   54]]\n",
      "  Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.86      0.92      1620\n",
      "           1       0.19      0.98      0.32        55\n",
      "\n",
      "    accuracy                           0.86      1675\n",
      "   macro avg       0.59      0.92      0.62      1675\n",
      "weighted avg       0.97      0.86      0.90      1675\n",
      "\n",
      "-----\n",
      "Decision Tree Results:\n",
      "  Accuracy: 0.9176\n",
      "  Balanced Accuracy: 0.9486\n",
      "  Precision: 0.2827\n",
      "  Recall: 0.9818\n",
      "  F1 Score: 0.4390\n",
      "  Training Time: 0.0039 seconds\n",
      "  Confusion Matrix:\n",
      "[[1483  137]\n",
      " [   1   54]]\n",
      "  Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.92      0.96      1620\n",
      "           1       0.28      0.98      0.44        55\n",
      "\n",
      "    accuracy                           0.92      1675\n",
      "   macro avg       0.64      0.95      0.70      1675\n",
      "weighted avg       0.98      0.92      0.94      1675\n",
      "\n",
      "-----\n",
      "SVM Results:\n",
      "  Accuracy: 0.8621\n",
      "  Balanced Accuracy: 0.9199\n",
      "  Precision: 0.1901\n",
      "  Recall: 0.9818\n",
      "  F1 Score: 0.3186\n",
      "  Training Time: 1.0964 seconds\n",
      "  Confusion Matrix:\n",
      "[[1390  230]\n",
      " [   1   54]]\n",
      "  Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.86      0.92      1620\n",
      "           1       0.19      0.98      0.32        55\n",
      "\n",
      "    accuracy                           0.86      1675\n",
      "   macro avg       0.59      0.92      0.62      1675\n",
      "weighted avg       0.97      0.86      0.90      1675\n",
      "\n",
      "-----\n",
      "Naive Bayes Results:\n",
      "  Accuracy: 0.9176\n",
      "  Balanced Accuracy: 0.9486\n",
      "  Precision: 0.2827\n",
      "  Recall: 0.9818\n",
      "  F1 Score: 0.4390\n",
      "  Training Time: 0.0015 seconds\n",
      "  Confusion Matrix:\n",
      "[[1483  137]\n",
      " [   1   54]]\n",
      "  Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.92      0.96      1620\n",
      "           1       0.28      0.98      0.44        55\n",
      "\n",
      "    accuracy                           0.92      1675\n",
      "   macro avg       0.64      0.95      0.70      1675\n",
      "weighted avg       0.98      0.92      0.94      1675\n",
      "\n",
      "-----\n",
      "\n",
      "----- Quantum Model (QNN) -----\n",
      "Qadence or other quantum packages not found: No module named 'qadence'\n",
      "Skipping quantum model.\n",
      "\n",
      "----- Optimized Quantum Evolution Kernel (QEK) Model -----\n",
      "Using pre-configured optimal parameters: mu=1.0, C=100.0, samples=1200, bit_depth=3\n",
      "Error in QEK model: No module named 'qek'\n",
      "Error in main function: tuple indices must be integers or slices, not str\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-1-af12138c6f5c>\", line 698, in train_and_evaluate_qek_model\n",
      "    from qek.kernel import QuantumEvolutionKernel as QEK\n",
      "ModuleNotFoundError: No module named 'qek'\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-1-af12138c6f5c>\", line 1169, in main\n",
      "    visualize_results(classical_results, quantum_results, qek_results, df)\n",
      "  File \"<ipython-input-1-af12138c6f5c>\", line 1030, in visualize_results\n",
      "    accuracies.append(quantum_results[\"accuracy\"])\n",
      "                      ~~~~~~~~~~~~~~~^^^^^^^^^^^^\n",
      "TypeError: tuple indices must be integers or slices, not str\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Google Colab version with online data access.\n",
    "\n",
    "This notebook demonstrates a comprehensive hybrid approach for predicting coral reef bleaching using a NOAA dataset.\n",
    "Instead of reading a local CSV, the dataset is loaded from an online source (GitHub).\n",
    "\"\"\"\n",
    "\n",
    "# =============================================================================\n",
    "# Section 1: Introduction & Setup\n",
    "# =============================================================================\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from math import pi as PI_value\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, confusion_matrix, classification_report,\n",
    "    precision_score, recall_score, f1_score, balanced_accuracy_score\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print(\"Coral Reef Bleaching Prediction using Classical and Quantum ML Approaches\")\n",
    "\n",
    "# =============================================================================\n",
    "# Section 2: Data Loading and Preprocessing (Online Access)\n",
    "# =============================================================================\n",
    "def load_and_preprocess_data():\n",
    "    \"\"\"\n",
    "    Load and preprocess the NOAA coral reef bleaching dataset from an online source.\n",
    "\n",
    "    Returns:\n",
    "        tuple: X_train_scaled, X_test_scaled, y_train, y_test, feature_names, df\n",
    "    \"\"\"\n",
    "    # Use the online CSV from GitHub (as used in other notebooks)\n",
    "    data_url = \"https://raw.githubusercontent.com/alyshapm/coral-reef-bleaching/main/dataset/NOAA_Reef_Check__Bleaching_Data.csv\"\n",
    "    print(\"Loading data from online URL:\", data_url)\n",
    "    df = pd.read_csv(data_url)\n",
    "    print(\"Original dataset shape:\", df.shape)\n",
    "    print(df.head())\n",
    "\n",
    "    # Clean the dataset: remove duplicates\n",
    "    df.drop_duplicates(inplace=True)\n",
    "\n",
    "    # Display class distribution before cleaning\n",
    "    if 'Bleaching' in df.columns:\n",
    "        bleaching_counts = df['Bleaching'].value_counts()\n",
    "        print(\"\\nClass distribution before cleaning:\")\n",
    "        print(bleaching_counts)\n",
    "        print(f\"Positive rate: {bleaching_counts.get('Yes', 0) / len(df):.2%}\")\n",
    "\n",
    "    # Standardize column names - replace spaces with underscores\n",
    "    df.columns = [col.replace(' ', '') for col in df.columns]\n",
    "\n",
    "    # Check for columns with all missing values\n",
    "    all_na_columns = [col for col in df.columns if df[col].isna().all()]\n",
    "    if all_na_columns:\n",
    "        print(f\"Dropping columns with all missing values: {all_na_columns}\")\n",
    "        df.drop(columns=all_na_columns, inplace=True)\n",
    "\n",
    "    # Handle missing values first: replace with mode for categorical, mean for numerical\n",
    "    for col in df.columns:\n",
    "        if df[col].isna().any():\n",
    "            print(f\"Handling missing values in {col} column: {df[col].isna().sum()} NaNs\")\n",
    "            if df[col].dtype == 'object':\n",
    "                # For categorical columns, fill with mode\n",
    "                if not df[col].dropna().empty:  # Only if there are non-NaN values\n",
    "                    mode_value = df[col].mode()[0]\n",
    "                    df[col].fillna(mode_value, inplace=True)\n",
    "                else:\n",
    "                    # If all values are NaN, fill with a default value\n",
    "                    print(f\"Column {col} has all NaN values, filling with 'none'\")\n",
    "                    df[col].fillna('none', inplace=True)\n",
    "            else:\n",
    "                # For numerical columns, fill with mean or 0 if all NaN\n",
    "                if not df[col].dropna().empty:  # Only if there are non-NaN values\n",
    "                    mean_value = df[col].mean()\n",
    "                    df[col].fillna(mean_value, inplace=True)\n",
    "                else:\n",
    "                    print(f\"Column {col} has all NaN values, filling with 0\")\n",
    "                    df[col].fillna(0, inplace=True)\n",
    "\n",
    "    print(f\"Shape after handling missing values: {df.shape}\")\n",
    "\n",
    "    # Display data types\n",
    "    print(\"\\nColumn data types:\")\n",
    "    print(df.dtypes)\n",
    "\n",
    "    print(\"\\nEncoding categorical columns...\")\n",
    "    # Encode all categorical columns\n",
    "\n",
    "    # Binary categorical columns - map Yes/No, yes/no to 1/0\n",
    "    binary_columns = [\"Bleaching\", \"Storms\"]\n",
    "    for col in binary_columns:\n",
    "        if col in df.columns:\n",
    "            # Handle case insensitivity\n",
    "            df[col] = df[col].astype(str).str.lower()\n",
    "            df[col] = df[col].map({\"yes\": 1, \"no\": 0})\n",
    "            print(f\"Encoded {col} to 1/0\")\n",
    "\n",
    "    # Ocean categorical column\n",
    "    if 'Ocean' in df.columns:\n",
    "        ocean_mapping = {\"Arabian Gulf\": 0, \"Atlantic\": 1, \"Indian\": 2, \"Pacific\": 3, \"Red Sea\": 4}\n",
    "        df['Ocean'] = df['Ocean'].map(ocean_mapping)\n",
    "        print(\"Encoded Ocean column\")\n",
    "\n",
    "    # Impact categorical columns with consistent naming\n",
    "    impact_columns = ['Commercial', 'HumanImpact', 'Siltation', 'Dynamite', 'Poison', 'Sewage', 'Industrial']\n",
    "    for col in impact_columns:\n",
    "        if col in df.columns:\n",
    "            # First, standardize values to lowercase and strip whitespace\n",
    "            df[col] = df[col].astype(str).str.lower().str.strip()\n",
    "\n",
    "            # Display unique values before mapping\n",
    "            unique_values = df[col].unique()\n",
    "            print(f\"Unique values in {col} before mapping: {unique_values}\")\n",
    "\n",
    "            # Map values\n",
    "            mapping = {'none': 0, 'low': 1, 'moderate': 2, 'high': 3, 'nan': 0}\n",
    "            df[col] = df[col].map(mapping)\n",
    "\n",
    "            # Check if mapping was successful\n",
    "            if df[col].isna().any():\n",
    "                print(f\"Warning: Column {col} has {df[col].isna().sum()} NaN values after mapping\")\n",
    "                print(f\"Unique values in {col} after mapping: {df[col].unique()}\")\n",
    "\n",
    "                # Fill NaN with 0 (representing 'none')\n",
    "                df[col].fillna(0, inplace=True)\n",
    "                print(f\"Filled NaN values in {col} with 0\")\n",
    "\n",
    "            print(f\"Encoded {col} column\")\n",
    "\n",
    "    # Check for any remaining object columns\n",
    "    object_columns = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    if object_columns:\n",
    "        print(f\"Warning: The following columns still have non-numeric types: {object_columns}\")\n",
    "        print(\"Converting remaining object columns to numeric if possible...\")\n",
    "\n",
    "        for col in object_columns:\n",
    "            try:\n",
    "                # Display unique values to help with debugging\n",
    "                print(f\"Unique values in {col}: {df[col].unique()}\")\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "                # Fill NaN with mean or mode\n",
    "                if df[col].isna().any():\n",
    "                    if df[col].nunique() > 10:  # If many unique values, use mean\n",
    "                        if not df[col].dropna().empty:\n",
    "                            df[col].fillna(df[col].mean(), inplace=True)\n",
    "                        else:\n",
    "                            df[col].fillna(0, inplace=True)\n",
    "                    else:  # If few unique values, use mode\n",
    "                        if not df[col].dropna().empty:\n",
    "                            df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "                        else:\n",
    "                            df[col].fillna(0, inplace=True)\n",
    "                print(f\"Converted {col} to numeric\")\n",
    "            except Exception as e:\n",
    "                print(f\"Could not convert {col} to numeric: {e}, dropping column\")\n",
    "                df.drop(columns=[col], inplace=True)\n",
    "\n",
    "    # Final check for NaN values\n",
    "    nan_check = df.isna().sum()\n",
    "    if nan_check.sum() > 0:\n",
    "        print(\"\\nWarning: Dataset still contains NaN values:\")\n",
    "        print(nan_check[nan_check > 0])\n",
    "        print(\"Filling remaining NaN values...\")\n",
    "        # Fill remaining NaN values with column means or 0\n",
    "        for col in df.columns:\n",
    "            if df[col].isna().any():\n",
    "                if df[col].dtype.kind in 'ifc':  # integer, float, complex\n",
    "                    if not df[col].dropna().empty:\n",
    "                        df[col].fillna(df[col].mean(), inplace=True)\n",
    "                    else:\n",
    "                        df[col].fillna(0, inplace=True)\n",
    "                else:\n",
    "                    df[col].fillna(0, inplace=True)  # Default to 0 for any type\n",
    "\n",
    "    # Compute correlation matrix and drop features with very low correlation with 'Bleaching'\n",
    "    try:\n",
    "        corr_matrix = df.corr()\n",
    "        low_corr_features = []\n",
    "        threshold = 0.1\n",
    "\n",
    "        # Print all correlations with Bleaching\n",
    "        print(\"\\nCorrelations with Bleaching:\")\n",
    "        for col in df.columns:\n",
    "            if col != 'Bleaching' and col in corr_matrix.index:\n",
    "                corr_value = abs(corr_matrix.loc[col, 'Bleaching'])\n",
    "                print(f\"Correlation of {col} with Bleaching: {corr_value:.4f}\")\n",
    "                if corr_value < threshold:\n",
    "                    low_corr_features.append(col)\n",
    "\n",
    "        if low_corr_features:\n",
    "            print(f\"Dropping low correlation features: {low_corr_features}\")\n",
    "            df.drop(columns=low_corr_features, inplace=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error computing correlations: {e}\")\n",
    "        print(\"Skipping correlation-based feature selection\")\n",
    "\n",
    "    # Define features and target\n",
    "    X = df.drop(columns=['Bleaching'])\n",
    "    y = df['Bleaching']\n",
    "    feature_names = X.columns.tolist()\n",
    "\n",
    "    # Check for class imbalance\n",
    "    print(\"\\nClass distribution after preprocessing:\")\n",
    "    print(y.value_counts())\n",
    "    print(f\"Positive rate: {y.mean():.2%}\")\n",
    "\n",
    "    # If the dataset is imbalanced, use stratified sampling\n",
    "    if y.mean() < 0.2:  # If positive class is less than 20%\n",
    "        print(\"Dataset is imbalanced. Using stratified sampling...\")\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.25, random_state=42, stratify=y\n",
    "        )\n",
    "    else:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.25, random_state=42\n",
    "        )\n",
    "\n",
    "    # Apply feature scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Verify no NaNs in the scaled data\n",
    "    if np.isnan(X_train_scaled).any() or np.isnan(X_test_scaled).any():\n",
    "        print(\"Warning: NaN values detected after scaling. Replacing with zeros...\")\n",
    "        X_train_scaled = np.nan_to_num(X_train_scaled)\n",
    "        X_test_scaled = np.nan_to_num(X_test_scaled)\n",
    "\n",
    "    print(f\"Training data shape: {X_train_scaled.shape}\")\n",
    "    print(f\"Test data shape: {X_test_scaled.shape}\")\n",
    "    print(f\"Training set positive rate: {y_train.mean():.2%}\")\n",
    "    print(f\"Test set positive rate: {y_test.mean():.2%}\")\n",
    "\n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test, feature_names, df\n",
    "\n",
    "# =============================================================================\n",
    "# Section 3: Classical Machine Learning Models\n",
    "# =============================================================================\n",
    "def train_and_evaluate_classical_models(X_train_scaled, X_test_scaled, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Train and evaluate classical machine learning models.\n",
    "\n",
    "    Args:\n",
    "        X_train_scaled: Scaled training features\n",
    "        X_test_scaled: Scaled test features\n",
    "        y_train: Training target\n",
    "        y_test: Test target\n",
    "\n",
    "    Returns:\n",
    "        dict: Results of classical models\n",
    "    \"\"\"\n",
    "    # Check for class imbalance and prepare model parameters accordingly\n",
    "    class_counts = np.bincount(y_train)\n",
    "    total_samples = len(y_train)\n",
    "    if len(class_counts) > 1:\n",
    "        minority_class_ratio = min(class_counts) / total_samples\n",
    "        print(f\"Minority class ratio: {minority_class_ratio:.2%}\")\n",
    "\n",
    "        # Adjust model parameters for imbalanced data if needed\n",
    "        imbalanced = minority_class_ratio < 0.2\n",
    "    else:\n",
    "        imbalanced = False\n",
    "        print(\"Warning: Only one class found in training data\")\n",
    "\n",
    "    # Define models with parameters adjusted for potential class imbalance\n",
    "    if imbalanced:\n",
    "        print(\"Using class weight 'balanced' for models due to imbalanced data\")\n",
    "        models = {\n",
    "            \"Logistic Regression\": LogisticRegression(\n",
    "                max_iter=1000,\n",
    "                random_state=42,\n",
    "                class_weight='balanced',\n",
    "                solver='liblinear'  # More stable with imbalanced data\n",
    "            ),\n",
    "            \"Decision Tree\": DecisionTreeClassifier(\n",
    "                random_state=42,\n",
    "                class_weight='balanced',\n",
    "                min_samples_leaf=5  # Prevent overfitting to minority class\n",
    "            ),\n",
    "            \"SVM\": SVC(\n",
    "                kernel='rbf',\n",
    "                probability=True,\n",
    "                random_state=42,\n",
    "                class_weight='balanced'\n",
    "            ),\n",
    "            \"Naive Bayes\": GaussianNB()  # No class_weight parameter for GaussianNB\n",
    "        }\n",
    "    else:\n",
    "        models = {\n",
    "            \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "            \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "            \"SVM\": SVC(kernel='rbf', probability=True, random_state=42),\n",
    "            \"Naive Bayes\": GaussianNB()\n",
    "        }\n",
    "\n",
    "    classical_results = {}\n",
    "\n",
    "    print(\"\\n----- Classical Model Results -----\")\n",
    "    for name, model in models.items():\n",
    "        try:\n",
    "            # Verify no NaN values\n",
    "            if np.isnan(X_train_scaled).any() or np.isnan(X_test_scaled).any():\n",
    "                print(f\"Warning: NaN values detected before training {name}. Replacing with zeros...\")\n",
    "                X_train_clean = np.nan_to_num(X_train_scaled)\n",
    "                X_test_clean = np.nan_to_num(X_test_scaled)\n",
    "            else:\n",
    "                X_train_clean = X_train_scaled\n",
    "                X_test_clean = X_test_scaled\n",
    "\n",
    "            start_time = time.time()\n",
    "            model.fit(X_train_clean, y_train)\n",
    "            training_time = time.time() - start_time\n",
    "\n",
    "            y_pred = model.predict(X_test_clean)\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "            # Calculate additional metrics for imbalanced data\n",
    "            precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "            recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "            f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "            balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "\n",
    "            classical_results[name] = {\n",
    "                \"accuracy\": accuracy,\n",
    "                \"balanced_accuracy\": balanced_acc,\n",
    "                \"precision\": precision,\n",
    "                \"recall\": recall,\n",
    "                \"f1_score\": f1,\n",
    "                \"confusion_matrix\": cm,\n",
    "                \"training_time\": training_time\n",
    "            }\n",
    "\n",
    "            print(f\"{name} Results:\")\n",
    "            print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "            print(f\"  Balanced Accuracy: {balanced_acc:.4f}\")\n",
    "            print(f\"  Precision: {precision:.4f}\")\n",
    "            print(f\"  Recall: {recall:.4f}\")\n",
    "            print(f\"  F1 Score: {f1:.4f}\")\n",
    "            print(f\"  Training Time: {training_time:.4f} seconds\")\n",
    "            print(f\"  Confusion Matrix:\\n{cm}\")\n",
    "            print(f\"  Classification Report:\\n{classification_report(y_test, y_pred, zero_division=0)}\")\n",
    "            print(\"-----\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error training {name}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "    return classical_results\n",
    "\n",
    "# =============================================================================\n",
    "# Section 4: Quantum Machine Learning Model using Qadence (QNN)\n",
    "# =============================================================================\n",
    "def train_and_evaluate_quantum_model(X_train_scaled, X_test_scaled, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Train and evaluate a quantum machine learning model using Qadence.\n",
    "\n",
    "    Args:\n",
    "        X_train_scaled: Scaled training features\n",
    "        X_test_scaled: Scaled test features\n",
    "        y_train: Training target\n",
    "        y_test: Test target\n",
    "\n",
    "    Returns:\n",
    "        tuple: Quantum model accuracy and training time\n",
    "    \"\"\"\n",
    "    print(\"\\n----- Quantum Model (QNN) -----\")\n",
    "    try:\n",
    "        # Import Qadence for constructing the quantum circuit\n",
    "        from qadence import QNN, QuantumCircuit, RX, RY, CNOT, Z, chain, FeatureParameter\n",
    "\n",
    "        # Check for NaN values and replace them\n",
    "        if np.isnan(X_train_scaled).any() or np.isnan(X_test_scaled).any():\n",
    "            print(\"Warning: NaN values detected. Replacing with zeros for quantum model.\")\n",
    "            X_train_scaled_clean = np.nan_to_num(X_train_scaled)\n",
    "            X_test_scaled_clean = np.nan_to_num(X_test_scaled)\n",
    "        else:\n",
    "            X_train_scaled_clean = X_train_scaled\n",
    "            X_test_scaled_clean = X_test_scaled\n",
    "\n",
    "        # For the quantum model, select features with highest correlation to target\n",
    "        if X_train_scaled_clean.shape[1] >= 2:\n",
    "            # Select the two features with highest correlation to target\n",
    "            if X_train_scaled_clean.shape[1] > 2:\n",
    "                # Calculate correlation with target for each feature\n",
    "                correlations = []\n",
    "                for i in range(X_train_scaled_clean.shape[1]):\n",
    "                    corr = np.corrcoef(X_train_scaled_clean[:, i], y_train)[0, 1]\n",
    "                    correlations.append((i, abs(corr)))\n",
    "\n",
    "                # Sort by absolute correlation and get top 2 feature indices\n",
    "                top_features = sorted(correlations, key=lambda x: x[1], reverse=True)[:2]\n",
    "                feature_indices = [idx for idx, _ in top_features]\n",
    "                print(f\"Using features with indices {feature_indices} for quantum model (highest correlations)\")\n",
    "            else:\n",
    "                feature_indices = [0, 1]\n",
    "                print(f\"Using the two available features for quantum model\")\n",
    "\n",
    "            X_train_q = X_train_scaled_clean[:, feature_indices]\n",
    "            X_test_q = X_test_scaled_clean[:, feature_indices]\n",
    "        else:\n",
    "            # If only one feature is available, duplicate it\n",
    "            X_train_q = np.hstack([X_train_scaled_clean, X_train_scaled_clean])\n",
    "            X_test_q = np.hstack([X_test_scaled_clean, X_test_scaled_clean])\n",
    "            print(f\"Using duplicated features for quantum model (only {X_train_scaled_clean.shape[1]} feature available)\")\n",
    "\n",
    "        # Normalize to [0, PI] for encoding as rotation angles\n",
    "        min_vals = X_train_q.min(axis=0)\n",
    "        max_vals = X_train_q.max(axis=0)\n",
    "        # Prevent division by zero\n",
    "        ranges = max_vals - min_vals\n",
    "        ranges[ranges == 0] = 1.0  # Replace zero ranges with 1.0\n",
    "\n",
    "        X_train_q_norm = (X_train_q - min_vals) / ranges * PI_value\n",
    "        X_test_q_norm = (X_test_q - min_vals) / ranges * PI_value\n",
    "\n",
    "        # Check for NaN or inf values after normalization\n",
    "        if np.isnan(X_train_q_norm).any() or np.isnan(X_test_q_norm).any() or \\\n",
    "           np.isinf(X_train_q_norm).any() or np.isinf(X_test_q_norm).any():\n",
    "            print(\"Warning: NaN or inf values detected after normalization. Replacing with PI/2.\")\n",
    "            X_train_q_norm = np.nan_to_num(X_train_q_norm, nan=PI_value/2, posinf=PI_value, neginf=0)\n",
    "            X_test_q_norm = np.nan_to_num(X_test_q_norm, nan=PI_value/2, posinf=PI_value, neginf=0)\n",
    "\n",
    "        # Convert target to numpy array if it's a pandas Series\n",
    "        if hasattr(y_train, 'values'):\n",
    "            y_train_np = y_train.values\n",
    "            y_test_np = y_test.values\n",
    "        else:\n",
    "            y_train_np = y_train\n",
    "            y_test_np = y_test\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        X_train_q_tensor = torch.tensor(X_train_q_norm, dtype=torch.float64)\n",
    "        y_train_tensor = torch.tensor(y_train_np, dtype=torch.float64).view(-1, 1)\n",
    "        X_test_q_tensor = torch.tensor(X_test_q_norm, dtype=torch.float64)\n",
    "        y_test_tensor = torch.tensor(y_test_np, dtype=torch.float64).view(-1, 1)\n",
    "\n",
    "        print(f\"Training tensor shape: {X_train_q_tensor.shape}\")\n",
    "        print(f\"Target tensor shape: {y_train_tensor.shape}\")\n",
    "\n",
    "        # Handle class imbalance by creating weighted sampling\n",
    "        # Calculate class weights\n",
    "        class_counts = np.bincount(y_train_np.astype(int))\n",
    "        weights = 1.0 / class_counts\n",
    "        sample_weights = torch.tensor([weights[t] for t in y_train_np.astype(int)], dtype=torch.float64)\n",
    "        sampler = torch.utils.data.WeightedRandomSampler(\n",
    "            weights=sample_weights,\n",
    "            num_samples=len(sample_weights),\n",
    "            replacement=True\n",
    "        )\n",
    "\n",
    "        # Create DataLoader with weighted sampling\n",
    "        batch_size = 128  # Using batches to improve training\n",
    "        dataset = torch.utils.data.TensorDataset(X_train_q_tensor, y_train_tensor)\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=batch_size,\n",
    "            sampler=sampler\n",
    "        )\n",
    "\n",
    "        # Build a 3-qubit quantum circuit with trainable parameters for better expressivity\n",
    "        from qadence import Parameter\n",
    "\n",
    "        # Create trainable parameters\n",
    "        theta1 = Parameter(\"theta1\")\n",
    "        theta2 = Parameter(\"theta2\")\n",
    "        theta3 = Parameter(\"theta3\")\n",
    "        theta4 = Parameter(\"theta4\")\n",
    "\n",
    "        # Build an enhanced quantum circuit with both feature encoding and trainable parameters\n",
    "        qnn_block = chain(\n",
    "            # Initial feature encoding layer\n",
    "            RX(0, FeatureParameter(\"phi\")),       # Encode first feature\n",
    "            RX(1, FeatureParameter(\"theta\")),     # Encode second feature\n",
    "            CNOT(0, 1),                           # Entangling gate\n",
    "\n",
    "            # First trainable layer\n",
    "            RY(0, theta1),                        # Trainable rotation\n",
    "            RY(1, theta2),                        # Trainable rotation\n",
    "            CNOT(1, 0),                           # Entangling gate\n",
    "\n",
    "            # Second trainable layer\n",
    "            RY(0, theta3),                        # Trainable rotation\n",
    "            RY(1, theta4),                        # Trainable rotation\n",
    "            CNOT(0, 1),                           # Entangling gate\n",
    "\n",
    "            # Feature re-encoding for better feature interaction\n",
    "            RY(0, FeatureParameter(\"phi\")),       # Encode first feature again\n",
    "            RY(1, FeatureParameter(\"theta\")),     # Encode second feature again\n",
    "            CNOT(0, 1)                            # Final entangling gate\n",
    "        )\n",
    "\n",
    "        qc = QuantumCircuit(2, qnn_block)\n",
    "        observable = Z(0)  # Measure qubit 0\n",
    "\n",
    "        # Create QNN with trainable parameters\n",
    "        qnn_model = QNN(\n",
    "            qc,\n",
    "            observable,\n",
    "            inputs=[\"phi\", \"theta\"]\n",
    "        )\n",
    "\n",
    "        # Print parameter names to verify\n",
    "        print(f\"QNN parameters: {qnn_model.parameters()}\")\n",
    "\n",
    "        if len(list(qnn_model.parameters())) == 0:\n",
    "            print(\"Warning: QNN has no trainable parameters. Using a different approach.\")\n",
    "\n",
    "            # Create a simpler fixed circuit for demonstration\n",
    "            qnn_model = None\n",
    "            quantum_accuracy = None\n",
    "            quantum_training_time = None\n",
    "        else:\n",
    "            # Wrap the QNN in a PyTorch Module for classification\n",
    "            class QuantumClassifier(nn.Module):\n",
    "                def __init__(self, qnn):\n",
    "                    super(QuantumClassifier, self).__init__()\n",
    "                    self.qnn = qnn\n",
    "                    # Add a classical layer to help with classification decision boundary\n",
    "                    self.post_process = nn.Sequential(\n",
    "                        nn.Linear(1, 4),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(4, 1),\n",
    "                        nn.Sigmoid()\n",
    "                    )\n",
    "\n",
    "                def forward(self, x):\n",
    "                    out = self.qnn(x).view(-1, 1)  # Expectation value in [-1, 1]\n",
    "                    # Process through classical layer for better classification\n",
    "                    return self.post_process(out)\n",
    "\n",
    "            quantum_classifier = QuantumClassifier(qnn_model)\n",
    "\n",
    "            # Loss and optimizer\n",
    "            # Use weighted binary cross-entropy loss to handle class imbalance\n",
    "            pos_weight = torch.tensor([class_counts[0] / class_counts[1]], dtype=torch.float64)\n",
    "            criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "            # Use a learning rate scheduler for better convergence\n",
    "            optimizer = optim.Adam(quantum_classifier.parameters(), lr=0.01)\n",
    "            scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer, mode='min', factor=0.5, patience=10, verbose=True\n",
    "            )\n",
    "\n",
    "            # Training loop for the QNN model\n",
    "            num_epochs = 200  # Increased number of epochs\n",
    "            quantum_training_start = time.time()\n",
    "\n",
    "            # Keep track of best model\n",
    "            best_f1 = 0\n",
    "            best_model_state = None\n",
    "\n",
    "            try:\n",
    "                for epoch in range(num_epochs):\n",
    "                    epoch_loss = 0\n",
    "                    # Train with batches\n",
    "                    for batch_x, batch_y in train_loader:\n",
    "                        optimizer.zero_grad()\n",
    "                        outputs = quantum_classifier(batch_x)\n",
    "                        loss = criterion(outputs, batch_y)\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        epoch_loss += loss.item()\n",
    "\n",
    "                    avg_loss = epoch_loss / len(train_loader)\n",
    "                    scheduler.step(avg_loss)\n",
    "\n",
    "                    # Evaluate every 20 epochs\n",
    "                    if (epoch+1) % 20 == 0:\n",
    "                        with torch.no_grad():\n",
    "                            q_outputs = quantum_classifier(X_test_q_tensor)\n",
    "                            q_preds = (q_outputs >= 0.5).float()\n",
    "                            # Make sure we're importing the correct f1_score from sklearn.metrics\n",
    "                            from sklearn.metrics import f1_score as sklearn_f1_score\n",
    "                            current_f1 = sklearn_f1_score(y_test_tensor.cpu().numpy().flatten(), q_preds.cpu().numpy().flatten(), average='binary')\n",
    "\n",
    "                            if current_f1 > best_f1:\n",
    "                                best_f1 = current_f1\n",
    "                                best_model_state = {k: v.clone() for k, v in quantum_classifier.state_dict().items()}\n",
    "\n",
    "                        print(f\"Quantum Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Current F1: {current_f1:.4f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error during quantum model training: {e}\")\n",
    "                print(\"Skipping further quantum model evaluation\")\n",
    "                return None, None\n",
    "\n",
    "            quantum_training_time = time.time() - quantum_training_start\n",
    "\n",
    "            # Load the best model state\n",
    "            if best_model_state:\n",
    "                quantum_classifier.load_state_dict(best_model_state)\n",
    "\n",
    "            # Evaluate QNN on test set\n",
    "            with torch.no_grad():\n",
    "                q_outputs = quantum_classifier(X_test_q_tensor)\n",
    "                q_preds = (q_outputs >= 0.5).float()\n",
    "                quantum_accuracy = (q_preds.eq(y_test_tensor).sum().item()) / y_test_tensor.size(0)\n",
    "\n",
    "            # Calculate additional metrics for imbalanced data\n",
    "            from sklearn.metrics import precision_score, recall_score, f1_score, balanced_accuracy_score\n",
    "\n",
    "            q_preds_np = q_preds.cpu().numpy().flatten()\n",
    "            y_test_np = y_test_tensor.cpu().numpy().flatten()\n",
    "\n",
    "            try:\n",
    "                precision = precision_score(y_test_np, q_preds_np, zero_division=0)\n",
    "                recall = recall_score(y_test_np, q_preds_np, zero_division=0)\n",
    "                f1 = f1_score(y_test_np, q_preds_np, zero_division=0)\n",
    "                balanced_acc = balanced_accuracy_score(y_test_np, q_preds_np)\n",
    "\n",
    "                print(f\"Quantum QNN Model Results:\")\n",
    "                print(f\"  Accuracy: {quantum_accuracy:.4f}\")\n",
    "                print(f\"  Balanced Accuracy: {balanced_acc:.4f}\")\n",
    "                print(f\"  Precision: {precision:.4f}\")\n",
    "                print(f\"  Recall: {recall:.4f}\")\n",
    "                print(f\"  F1 Score: {f1:.4f}\")\n",
    "                print(f\"  Training Time: {quantum_training_time:.4f} seconds\")\n",
    "                print(f\"  Confusion Matrix:\\n{confusion_matrix(y_test_np, q_preds_np)}\")\n",
    "                print(f\"  Classification Report:\\n{classification_report(y_test_np, q_preds_np, zero_division=0)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error calculating quantum model metrics: {e}\")\n",
    "\n",
    "            # Create a results dictionary similar to classical models\n",
    "            quantum_results = None\n",
    "            if quantum_accuracy is not None:\n",
    "                quantum_results = {\n",
    "                    \"accuracy\": quantum_accuracy,\n",
    "                    \"balanced_accuracy\": balanced_acc,\n",
    "                    \"precision\": precision,\n",
    "                    \"recall\": recall,\n",
    "                    \"f1_score\": f1,\n",
    "                    \"confusion_matrix\": confusion_matrix(y_test_np, q_preds_np),\n",
    "                    \"training_time\": quantum_training_time\n",
    "                }\n",
    "\n",
    "        return quantum_results\n",
    "\n",
    "    except ImportError as e:\n",
    "        print(f\"Qadence or other quantum packages not found: {e}\")\n",
    "        print(\"Skipping quantum model.\")\n",
    "        return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"Error in quantum model: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "# =============================================================================\n",
    "# Section 5: Quantum Evolution Kernel (QEK) Based Model\n",
    "# =============================================================================\n",
    "def train_and_evaluate_qek_model(X_train_scaled, X_test_scaled, y_train, y_test, feature_names=None):\n",
    "    \"\"\"\n",
    "    Train and evaluate a model using the Quantum Evolution Kernel (QEK).\n",
    "    Preconfigured with optimal parameters from grid search.\n",
    "\n",
    "    Args:\n",
    "        X_train_scaled: Scaled training features\n",
    "        X_test_scaled: Scaled test features\n",
    "        y_train: Training target\n",
    "        y_test: Test target\n",
    "        feature_names: Optional list of feature names\n",
    "\n",
    "    Returns:\n",
    "        dict: Results from QEK model\n",
    "    \"\"\"\n",
    "    print(\"\\n----- Optimized Quantum Evolution Kernel (QEK) Model -----\")\n",
    "    print(\"Using pre-configured optimal parameters: mu=1.0, C=100.0, samples=1200, bit_depth=3\")\n",
    "\n",
    "    # Import required libraries\n",
    "    import numpy as np\n",
    "    import time\n",
    "    import traceback\n",
    "    from sklearn.metrics import (\n",
    "        accuracy_score, confusion_matrix, classification_report,\n",
    "        precision_score, recall_score, f1_score, balanced_accuracy_score\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Try to import QEK\n",
    "        from qek.kernel import QuantumEvolutionKernel as QEK\n",
    "        print(\"QEK library successfully imported\")\n",
    "\n",
    "        # Create a class for QEK data points\n",
    "        class GraphStructuredDataPoint:\n",
    "            \"\"\"\n",
    "            Custom class with state_dict attribute for QEK.\n",
    "            \"\"\"\n",
    "            def __init__(self, state_dict, target=None):\n",
    "                self.state_dict = state_dict\n",
    "                self.target = target\n",
    "\n",
    "        # Check for NaN values\n",
    "        if np.isnan(X_train_scaled).any() or np.isnan(X_test_scaled).any():\n",
    "            print(\"Warning: NaN values detected. Replacing with zeros.\")\n",
    "            X_train_scaled_clean = np.nan_to_num(X_train_scaled)\n",
    "            X_test_scaled_clean = np.nan_to_num(X_test_scaled)\n",
    "        else:\n",
    "            X_train_scaled_clean = X_train_scaled\n",
    "            X_test_scaled_clean = X_test_scaled\n",
    "\n",
    "        # Feature to state dictionary conversion with optimal bit depth\n",
    "        def feature_to_state_dict(features):\n",
    "            \"\"\"\n",
    "            Convert feature vector to a quantum state dictionary.\n",
    "            Using bit_depth=3 based on grid search results.\n",
    "            \"\"\"\n",
    "            bit_depth = 3  # Optimal value from grid search\n",
    "\n",
    "            # Ensure features are positive (for probability distribution)\n",
    "            if np.any(features < 0):\n",
    "                # Shift distribution to be positive\n",
    "                features = features - np.min(features)\n",
    "\n",
    "            # Get feature magnitudes\n",
    "            features_abs = np.abs(features)\n",
    "            sum_features = np.sum(features_abs)\n",
    "\n",
    "            # Normalize to valid probability distribution\n",
    "            if sum_features < 1e-10:\n",
    "                # Handle zero vectors\n",
    "                probs = np.ones_like(features_abs) / len(features_abs)\n",
    "            else:\n",
    "                probs = features_abs / sum_features\n",
    "\n",
    "            # Create state dictionary\n",
    "            state_dict = {}\n",
    "\n",
    "            # Get indices and sort by probability (highest first)\n",
    "            idx_prob_pairs = [(i, p) for i, p in enumerate(probs)]\n",
    "            idx_prob_pairs.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "            # Take top K probabilities or all if fewer\n",
    "            K = min(2**bit_depth - 1, len(features))\n",
    "            total_prob = 0.0\n",
    "\n",
    "            # Create binary representation for top K probabilities\n",
    "            for i, (idx, prob) in enumerate(idx_prob_pairs[:K]):\n",
    "                if prob > 1e-10:  # Only keep non-negligible probabilities\n",
    "                    # Generate binary string of appropriate length\n",
    "                    binary = format(i, f'0{bit_depth}b')\n",
    "                    state_dict[binary] = float(prob)\n",
    "                    total_prob += prob\n",
    "\n",
    "            # Ensure dictionary isn't empty\n",
    "            if not state_dict:\n",
    "                state_dict['0' * bit_depth] = 1.0\n",
    "            elif total_prob < 0.99:\n",
    "                # Normalize to ensure probabilities sum to 1\n",
    "                for key in state_dict:\n",
    "                    state_dict[key] /= total_prob\n",
    "\n",
    "            return state_dict\n",
    "\n",
    "        # Create dataset of appropriate size (1200 samples based on grid search)\n",
    "        from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "        max_samples = 1200  # Optimal value from grid search\n",
    "\n",
    "        if len(X_train_scaled_clean) > max_samples:\n",
    "            print(f\"Creating dataset with {max_samples} samples (optimal size from grid search)\")\n",
    "            sss = StratifiedShuffleSplit(n_splits=1, test_size=1-max_samples/len(X_train_scaled_clean), random_state=42)\n",
    "            for train_idx, _ in sss.split(X_train_scaled_clean, y_train):\n",
    "                X_train_reduced = X_train_scaled_clean[train_idx]\n",
    "                y_train_reduced = np.array(y_train)[train_idx]\n",
    "        else:\n",
    "            X_train_reduced = X_train_scaled_clean\n",
    "            y_train_reduced = np.array(y_train)\n",
    "\n",
    "        # Similarly, limit test set size for efficient computation\n",
    "        max_test_samples = min(200, len(X_test_scaled_clean))\n",
    "        if len(X_test_scaled_clean) > max_test_samples:\n",
    "            print(f\"Using {max_test_samples} test samples for efficient computation\")\n",
    "            sss = StratifiedShuffleSplit(n_splits=1, test_size=1-max_test_samples/len(X_test_scaled_clean), random_state=42)\n",
    "            for test_idx, _ in sss.split(X_test_scaled_clean, y_test):\n",
    "                X_test_reduced = X_test_scaled_clean[test_idx]\n",
    "                y_test_reduced = np.array(y_test)[test_idx]\n",
    "        else:\n",
    "            X_test_reduced = X_test_scaled_clean\n",
    "            y_test_reduced = np.array(y_test)\n",
    "\n",
    "        print(f\"Using {len(X_train_reduced)} training samples and {len(X_test_reduced)} test samples\")\n",
    "\n",
    "        # Convert data to QEK format with state dictionaries\n",
    "        train_data = [\n",
    "            GraphStructuredDataPoint(feature_to_state_dict(x), y)\n",
    "            for x, y in zip(X_train_reduced, y_train_reduced)\n",
    "        ]\n",
    "\n",
    "        test_data = [\n",
    "            GraphStructuredDataPoint(feature_to_state_dict(x), y)\n",
    "            for x, y in zip(X_test_reduced, y_test_reduced)\n",
    "        ]\n",
    "\n",
    "        # Print sample state dictionary\n",
    "        print(f\"Sample state_dict format: {train_data[0].state_dict}\")\n",
    "\n",
    "        # Initialize QEK with optimal mu=1.0 from grid search\n",
    "        print(\"Initializing QEK with optimal =1.0\")\n",
    "        start_time = time.time()\n",
    "        kernel = QEK(mu=1.0)  # Optimal value from grid search\n",
    "\n",
    "        # Compute kernel matrices in batches\n",
    "        print(\"Computing kernel matrices...\")\n",
    "        batch_size = 50  # Use batches for memory efficiency\n",
    "\n",
    "        n_train = len(train_data)\n",
    "        n_test = len(test_data)\n",
    "\n",
    "        # Initialize kernel matrices\n",
    "        K_train = np.zeros((n_train, n_train))\n",
    "        K_test = np.zeros((n_test, n_train))\n",
    "\n",
    "        # Compute training kernel matrix in batches with progress reporting\n",
    "        print(\"Generating training kernel matrix...\")\n",
    "        for i in range(0, n_train, batch_size):\n",
    "            print(f\"  Processing batch {i//batch_size + 1}/{(n_train-1)//batch_size + 1}\")\n",
    "            batch_end = min(i + batch_size, n_train)\n",
    "\n",
    "            for j in range(0, n_train, batch_size):\n",
    "                j_end = min(j + batch_size, n_train)\n",
    "\n",
    "                # Compute kernel values for this batch\n",
    "                for bi in range(i, batch_end):\n",
    "                    for bj in range(j, j_end):\n",
    "                        try:\n",
    "                            K_train[bi, bj] = kernel(train_data[bi], train_data[bj])\n",
    "                            # Ensure symmetry for faster computation\n",
    "                            if bi != bj:\n",
    "                                K_train[bj, bi] = K_train[bi, bj]\n",
    "                        except Exception as ke:\n",
    "                            # Fallback for any kernel computation errors\n",
    "                            print(f\"  Kernel computation error at ({bi},{bj}): {ke}\")\n",
    "                            # Default to identity kernel in case of error\n",
    "                            K_train[bi, bj] = 1.0 if bi == bj else 0.0\n",
    "                            if bi != bj:\n",
    "                                K_train[bj, bi] = K_train[bi, bj]\n",
    "\n",
    "        # Compute test kernel matrix in batches\n",
    "        print(\"Generating test kernel matrix...\")\n",
    "        for i in range(0, n_test, batch_size):\n",
    "            print(f\"  Processing test batch {i//batch_size + 1}/{(n_test-1)//batch_size + 1}\")\n",
    "            batch_end = min(i + batch_size, n_test)\n",
    "\n",
    "            for j in range(0, n_train, batch_size):\n",
    "                j_end = min(j + batch_size, n_train)\n",
    "\n",
    "                # Compute kernel values for this batch\n",
    "                for bi in range(i, batch_end):\n",
    "                    for bj in range(j, j_end):\n",
    "                        try:\n",
    "                            K_test[bi, bj] = kernel(test_data[bi], train_data[bj])\n",
    "                        except Exception as ke:\n",
    "                            # Fallback for any kernel computation errors\n",
    "                            print(f\"  Kernel computation error at test point ({bi},{bj}): {ke}\")\n",
    "                            # Default to zero for test points in case of error\n",
    "                            K_test[bi, bj] = 0.0\n",
    "\n",
    "        # Check for NaN or infinity values in kernel matrices\n",
    "        if np.isnan(K_train).any() or np.isinf(K_train).any():\n",
    "            print(\"Warning: NaN or Inf values in training kernel matrix. Replacing with zeros.\")\n",
    "            K_train = np.nan_to_num(K_train)\n",
    "\n",
    "        if np.isnan(K_test).any() or np.isinf(K_test).any():\n",
    "            print(\"Warning: NaN or Inf values in test kernel matrix. Replacing with zeros.\")\n",
    "            K_test = np.nan_to_num(K_test)\n",
    "\n",
    "        # Train SVM with precomputed kernel using optimal C=100.0 from grid search\n",
    "        print(\"Training SVM with precomputed kernel matrix (C=100.0)...\")\n",
    "        from sklearn.svm import SVC\n",
    "\n",
    "        model = SVC(\n",
    "            kernel='precomputed',\n",
    "            random_state=42,\n",
    "            class_weight='balanced',\n",
    "            C=100.0  # Optimal value from grid search\n",
    "        )\n",
    "\n",
    "        # Train with precomputed kernel matrix\n",
    "        try:\n",
    "            model.fit(K_train, y_train_reduced)\n",
    "\n",
    "            # Predict using precomputed kernel matrix\n",
    "            y_pred_qek = model.predict(K_test)\n",
    "            qek_training_time = time.time() - start_time\n",
    "\n",
    "            # Calculate evaluation metrics\n",
    "            accuracy = accuracy_score(y_test_reduced, y_pred_qek)\n",
    "            balanced_acc = balanced_accuracy_score(y_test_reduced, y_pred_qek)\n",
    "            f1 = f1_score(y_test_reduced, y_pred_qek, average='weighted', zero_division=0)\n",
    "            precision = precision_score(y_test_reduced, y_pred_qek, average='weighted', zero_division=0)\n",
    "            recall = recall_score(y_test_reduced, y_pred_qek, average='weighted', zero_division=0)\n",
    "\n",
    "            # Print metrics\n",
    "            print(\"\\nQEK-SVM Model Results:\")\n",
    "            print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "            print(f\"  Balanced Accuracy: {balanced_acc:.4f}\")\n",
    "            print(f\"  Precision: {precision:.4f}\")\n",
    "            print(f\"  Recall: {recall:.4f}\")\n",
    "            print(f\"  F1 Score: {f1:.4f}\")\n",
    "            print(f\"  Training Time: {qek_training_time:.4f} seconds\")\n",
    "            print(f\"  Confusion Matrix:\\n{confusion_matrix(y_test_reduced, y_pred_qek)}\")\n",
    "            print(f\"  Classification Report:\\n{classification_report(y_test_reduced, y_pred_qek, zero_division=0)}\")\n",
    "\n",
    "            # Store results\n",
    "            qek_results = {\n",
    "                \"accuracy\": accuracy,\n",
    "                \"balanced_accuracy\": balanced_acc,\n",
    "                \"precision\": precision,\n",
    "                \"recall\": recall,\n",
    "                \"f1_score\": f1,\n",
    "                \"confusion_matrix\": confusion_matrix(y_test_reduced, y_pred_qek),\n",
    "                \"training_time\": qek_training_time,\n",
    "                \"parameters\": {\"mu\": 1.0, \"C\": 100.0, \"samples\": max_samples, \"bit_depth\": 3}\n",
    "            }\n",
    "\n",
    "            return qek_results\n",
    "\n",
    "        except Exception as fit_error:\n",
    "            print(f\"Error fitting SVM with QEK kernel: {fit_error}\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "            # If kernel matrix is invalid, attempt regularization\n",
    "            print(\"Attempting to regularize kernel matrix...\")\n",
    "            # Add small diagonal term to ensure positive definiteness\n",
    "            np.fill_diagonal(K_train, np.diag(K_train) + 1e-10)\n",
    "\n",
    "            try:\n",
    "                model.fit(K_train, y_train_reduced)\n",
    "                y_pred_qek = model.predict(K_test)\n",
    "                qek_training_time = time.time() - start_time\n",
    "\n",
    "                # Calculate evaluation metrics\n",
    "                accuracy = accuracy_score(y_test_reduced, y_pred_qek)\n",
    "                balanced_acc = balanced_accuracy_score(y_test_reduced, y_pred_qek)\n",
    "                f1 = f1_score(y_test_reduced, y_pred_qek, average='weighted', zero_division=0)\n",
    "                precision = precision_score(y_test_reduced, y_pred_qek, average='weighted', zero_division=0)\n",
    "                recall = recall_score(y_test_reduced, y_pred_qek, average='weighted', zero_division=0)\n",
    "\n",
    "                # Print metrics\n",
    "                print(\"\\nQEK-SVM Model Results (after regularization):\")\n",
    "                print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "                print(f\"  Balanced Accuracy: {balanced_acc:.4f}\")\n",
    "                print(f\"  Precision: {precision:.4f}\")\n",
    "                print(f\"  Recall: {recall:.4f}\")\n",
    "                print(f\"  F1 Score: {f1:.4f}\")\n",
    "                print(f\"  Training Time: {qek_training_time:.4f} seconds\")\n",
    "\n",
    "                # Store results\n",
    "                qek_results = {\n",
    "                    \"accuracy\": accuracy,\n",
    "                    \"balanced_accuracy\": balanced_acc,\n",
    "                    \"precision\": precision,\n",
    "                    \"recall\": recall,\n",
    "                    \"f1_score\": f1,\n",
    "                    \"confusion_matrix\": confusion_matrix(y_test_reduced, y_pred_qek),\n",
    "                    \"training_time\": qek_training_time,\n",
    "                    \"parameters\": {\"mu\": 1.0, \"C\": 100.0, \"samples\": max_samples, \"bit_depth\": 3}\n",
    "                }\n",
    "\n",
    "                return qek_results\n",
    "\n",
    "            except Exception as reg_error:\n",
    "                print(f\"Regularization attempt failed: {reg_error}\")\n",
    "                return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in QEK model: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Section 6: Comparison and Visualization\n",
    "# =============================================================================\n",
    "def visualize_results(classical_results,quantum_results , qek_results, df):\n",
    "    \"\"\"\n",
    "    Visualize and compare model results.\n",
    "\n",
    "    Args:\n",
    "        classical_results: Results from classical models\n",
    "        quantum_accuracy: Accuracy of quantum model\n",
    "        quantum_training_time: Training time of quantum model\n",
    "        qek_results: Results from QEK model\n",
    "        df: Preprocessed dataframe\n",
    "    \"\"\"\n",
    "    if not classical_results and quantum_results is None and qek_results is None:\n",
    "        print(\"No results to visualize.\")\n",
    "        return\n",
    "\n",
    "    model_names = []\n",
    "    accuracies = []\n",
    "    balanced_accs = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "    training_times = []\n",
    "\n",
    "    # Collect results from classical models\n",
    "    if classical_results:\n",
    "        for name, result in classical_results.items():\n",
    "            model_names.append(name)\n",
    "            accuracies.append(result[\"accuracy\"])\n",
    "            balanced_accs.append(result[\"balanced_accuracy\"])\n",
    "            precisions.append(result[\"precision\"])\n",
    "            recalls.append(result[\"recall\"])\n",
    "            f1_scores.append(result[\"f1_score\"])\n",
    "            training_times.append(result[\"training_time\"])\n",
    "\n",
    "    # Add Qadence QNN results if available\n",
    "    if quantum_results is not None:\n",
    "        model_names.append(\"Quantum QNN\")\n",
    "        accuracies.append(quantum_results[\"accuracy\"])\n",
    "        balanced_accs.append(quantum_results[\"balanced_accuracy\"])\n",
    "        precisions.append(quantum_results[\"precision\"])\n",
    "        recalls.append(quantum_results[\"recall\"])\n",
    "        f1_scores.append(quantum_results[\"f1_score\"])\n",
    "        training_times.append(quantum_results[\"training_time\"])\n",
    "\n",
    "    # Add QEK results if available\n",
    "    if qek_results is not None:\n",
    "        model_names.append(\"QEK-SVM\")\n",
    "        accuracies.append(qek_results[\"accuracy\"])\n",
    "        balanced_accs.append(qek_results[\"balanced_accuracy\"])\n",
    "        precisions.append(qek_results[\"precision\"])\n",
    "        recalls.append(qek_results[\"recall\"])\n",
    "        f1_scores.append(qek_results[\"f1_score\"])\n",
    "        training_times.append(qek_results[\"training_time\"])\n",
    "\n",
    "    # 1. Plot model accuracies\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bars = plt.bar(model_names, accuracies, color='skyblue')\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Model Accuracy Comparison\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    # Add accuracy values on top of bars\n",
    "    for bar, acc in zip(bars, accuracies):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                 f'{acc:.4f}', ha='center', va='bottom')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"model_accuracy_comparison.png\")\n",
    "    plt.show()\n",
    "\n",
    "    # 2. Plot balanced accuracies (better for imbalanced data)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bars = plt.bar(model_names, balanced_accs, color='lightgreen')\n",
    "    plt.ylabel(\"Balanced Accuracy\")\n",
    "    plt.title(\"Model Balanced Accuracy Comparison\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    # Add balanced accuracy values on top of bars\n",
    "    for bar, bacc in zip(bars, balanced_accs):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                 f'{bacc:.4f}', ha='center', va='bottom')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"model_balanced_accuracy_comparison.png\")\n",
    "    plt.show()\n",
    "\n",
    "    # 3. Plot F1 scores\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bars = plt.bar(model_names, f1_scores, color='coral')\n",
    "    plt.ylabel(\"F1 Score\")\n",
    "    plt.title(\"Model F1 Score Comparison\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    # Add F1 score values on top of bars\n",
    "    for bar, f1 in zip(bars, f1_scores):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                 f'{f1:.4f}', ha='center', va='bottom')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"model_f1_score_comparison.png\")\n",
    "    plt.show()\n",
    "\n",
    "    # 4. Plot training times\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bars = plt.bar(model_names, training_times, color='lightpink')\n",
    "    plt.ylabel(\"Training Time (seconds)\")\n",
    "    plt.title(\"Model Training Time Comparison\")\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    # Add time values on top of bars\n",
    "    for bar, time_val in zip(bars, training_times):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                 f'{time_val:.2f}s', ha='center', va='bottom')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"model_training_time_comparison.png\")\n",
    "    plt.show()\n",
    "\n",
    "    # 5. Combined metrics chart\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    x = np.arange(len(model_names))\n",
    "    width = 0.2\n",
    "\n",
    "    plt.bar(x - width*1.5, accuracies, width, label='Accuracy', color='skyblue')\n",
    "    plt.bar(x - width/2, balanced_accs, width, label='Balanced Accuracy', color='lightgreen')\n",
    "    plt.bar(x + width/2, precisions, width, label='Precision', color='coral')\n",
    "    plt.bar(x + width*1.5, recalls, width, label='Recall', color='lightpink')\n",
    "\n",
    "    plt.xlabel('Models')\n",
    "    plt.ylabel('Scores')\n",
    "    plt.title('All Metrics Comparison')\n",
    "    plt.xticks(x, model_names, rotation=45)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"all_metrics_comparison.png\")\n",
    "    plt.show()\n",
    "\n",
    "    # 6. Display a correlation heatmap\n",
    "    try:\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        corr_matrix = df.corr()\n",
    "        mask = np.triu(np.ones_like(corr_matrix, dtype=bool))  # Mask for upper triangle\n",
    "\n",
    "        sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\",\n",
    "                    linewidths=0.5, mask=mask)\n",
    "        plt.title(\"Feature Correlation Heatmap\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"feature_correlation_heatmap.png\")\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating correlation heatmap: {e}\")\n",
    "\n",
    "# =============================================================================\n",
    "# Main Function\n",
    "# =============================================================================\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the coral reef bleaching prediction pipeline.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. Load and preprocess data\n",
    "        X_train_scaled, X_test_scaled, y_train, y_test, feature_names, df = load_and_preprocess_data()\n",
    "\n",
    "        # 2. Train and evaluate classical models\n",
    "        classical_results = train_and_evaluate_classical_models(X_train_scaled, X_test_scaled, y_train, y_test)\n",
    "\n",
    "        # 3. Train and evaluate quantum model (QNN)\n",
    "        quantum_results = train_and_evaluate_quantum_model(X_train_scaled, X_test_scaled, y_train, y_test)\n",
    "\n",
    "        # 4. Train and evaluate QEK model\n",
    "        qek_results = train_and_evaluate_qek_model(X_train_scaled, X_test_scaled, y_train, y_test, feature_names)\n",
    "\n",
    "        # 5. Visualize results\n",
    "        visualize_results(classical_results, quantum_results, qek_results, df)\n",
    "\n",
    "        # 6. Print final summary\n",
    "        print(\"\\n----- Final Summary -----\")\n",
    "        print(\"Top performing models:\")\n",
    "        all_accuracies = []\n",
    "\n",
    "        for name, result in classical_results.items():\n",
    "            all_accuracies.append((name, result[\"accuracy\"]))\n",
    "\n",
    "        if quantum_results is not None:\n",
    "            all_accuracies.append((\"Quantum QNN\", quantum_results[\"accuracy\"]))\n",
    "\n",
    "        if qek_results is not None:\n",
    "            all_accuracies.append((\"QEK-SVM\", qek_results[\"accuracy\"]))\n",
    "\n",
    "        # Sort by accuracy (descending)\n",
    "        all_accuracies.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        for i, (name, acc) in enumerate(all_accuracies):\n",
    "            print(f\"{i+1}. {name}: {acc:.4f}\")\n",
    "\n",
    "        print(\"\\nFigures saved:\")\n",
    "        print(\"- model_accuracy_comparison.png\")\n",
    "        print(\"- model_balanced_accuracy_comparison.png\")\n",
    "        print(\"- model_f1_score_comparison.png\")\n",
    "        print(\"- model_training_time_comparison.png\")\n",
    "        print(\"- all_metrics_comparison.png\")\n",
    "        print(\"- feature_correlation_heatmap.png\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in main function: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
